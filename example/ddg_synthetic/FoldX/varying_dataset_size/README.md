## Graphinity inference: test models developed with varying train + validation dataset sizes

### Data
Download the parquet files from https://opig.stats.ox.ac.uk/data/downloads/affinity_dataset/Synthetic_FoldX_ddG-varying_dataset_size-test-parquets.tar.gz (21 GB). Untarring will create a pdb_wt/ (54 MB) and pdb_mut/ (33 GB) directory. Place these in the example/ddg_synthetic/FoldX/varying_dataset_size/data/ directory:
 - WT PDB parquets: example/ddg_synthetic/FoldX/varying_dataset_size/data/pdb_wt/
 - Mutant PDB parquets: example/ddg_synthetic/FoldX/varying_dataset_size/data/pdb_mut/

These parquet files correspond to the PDBs in the Synthetic_FoldX_ddG-varying_dataset_size-test.csv file. The parquet files were generated by processing (atom typing) the input PDB files using src/ddg_regression/base/dataset.py. The parquet files have a smaller file size than the corresponding PDB files and inference is faster when starting with pre-processed files.

To note, the code is compatible with PDB (rather than parquet) inputs as well; no changes need to be made to the code or config.

### Running Graphinity inference

From the root directory of the repository, run:
```
python3 src/ddg_regression/graphinity_inference.py -c example/ddg_synthetic/FoldX/varying_dataset_size/configs/config-test_varying_amounts-{dataset}.yaml
```
e.g.,
```
python3 src/ddg_regression/graphinity_inference.py -c example/ddg_synthetic/FoldX/varying_dataset_size/configs/config-test_varying_amounts-subset_90000.yaml
```

Each test took approximately 40 minutes to run on a Linux machine with 1 GPU (NVIDIA RTX 6000) and 1 CPU.

The expected outputs are included in the outputs/ directory.
